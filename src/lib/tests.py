from text import normalize, top_n, count_freq, tokenize


def test_normalize():
    assert normalize("–ü—Ä–ò–≤–ï—Ç\n–ú–ò—Ä\t") == "–ø—Ä–∏–≤–µ—Ç –º–∏—Ä"
    assert normalize("—ë–∂–∏–∫, –Å–ª–∫–∞", yo2e=True) == "–µ–∂–∏–∫, –µ–ª–∫–∞"
    assert normalize("Hello\r\nWorld") == "hello world"
    assert normalize("  –¥–≤–æ–π–Ω—ã–µ   –ø—Ä–æ–±–µ–ª—ã  ") == "–¥–≤–æ–π–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã"


def test_top_n():
    assert top_n({"a": 3, "b": 2, "c": 1}, n=2) == [("a", 3), ("b", 2)]
    assert top_n({"aa": 2, "bb": 2, "cc": 1}, n=2) == [("aa", 2), ("bb", 2)]


def test_count_freq():
    assert count_freq(["a", "b", "a", "c", "b", "a"]) == {"a": 3, "b": 2, "c": 1}
    assert count_freq(["bb", "aa", "bb", "aa", "cc"]) == {"aa": 2, "bb": 2, "cc": 1}


def test_tokenize():
    assert tokenize("–ø—Ä–∏–≤–µ—Ç –º–∏—Ä") == ["–ø—Ä–∏–≤–µ—Ç", "–º–∏—Ä"]
    assert tokenize("hello,world!!!") == ["hello", "world"]
    assert tokenize("–ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É –∫—Ä—É—Ç–æ") == ["–ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É", "–∫—Ä—É—Ç–æ"]
    assert tokenize("2025 –≥–æ–¥") == ["2025", "–≥–æ–¥"]
    assert tokenize("emoji üòÄ –Ω–µ —Å–ª–æ–≤–æ") == ["emoji", "–Ω–µ", "—Å–ª–æ–≤–æ"]


def main():
    test_normalize()
    test_top_n()
    test_count_freq()
    test_tokenize()


if __name__ == "__main__":
    main()
