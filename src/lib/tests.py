from text import normalize, top_n, count_freq, tokenize


def test_normalize():
    assert normalize("–ü—Ä–ò–≤–ï—Ç\n–ú–ò—Ä\t") == "–ø—Ä–∏–≤–µ—Ç –º–∏—Ä"
    assert normalize("—ë–∂–∏–∫, –Å–ª–∫–∞", yo2e=True) == "–µ–∂–∏–∫, –µ–ª–∫–∞"
    assert normalize("Hello\r\nWorld") == "hello world"
    assert normalize( "  –¥–≤–æ–π–Ω—ã–µ   –ø—Ä–æ–±–µ–ª—ã  ") == "–¥–≤–æ–π–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã"

def test_top_n():
    assert top_n({"a":3,"b":2,"c":1}, n=2) == [("a",3), ("b",2)]
    assert top_n({"aa":2,"bb":2,"cc":1}, n=2) == [("aa",2), ("bb",2)]


def test_count_freq():
    assert count_freq(["a","b","a","c","b","a"]) =={"a": 3, "b": 2, "c": 1}
    assert count_freq(["bb","aa","bb","aa","cc"]) == {"aa": 2, "bb": 2, "cc": 1}

def test_tokenize():
    assert tokenize("–ø—Ä–∏–≤–µ—Ç –º–∏—Ä") == ["–ø—Ä–∏–≤–µ—Ç", "–º–∏—Ä"]
    assert tokenize("hello,world!!!") == ["hello", "world"]
    assert tokenize("–ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É –∫—Ä—É—Ç–æ") == ["–ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É", "–∫—Ä—É—Ç–æ"]
    assert tokenize("2025 –≥–æ–¥") == ["2025", "–≥–æ–¥"]
    assert tokenize("emoji üòÄ –Ω–µ —Å–ª–æ–≤–æ") == ["emoji", "–Ω–µ", "—Å–ª–æ–≤–æ"]

def main():
    test_normalize()
    test_top_n()
    test_count_freq()
    test_tokenize()

if __name__ == "__main__":
    main()